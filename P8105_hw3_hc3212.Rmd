---
title: "P8105_hw4_hc3212"
author: "Hening cui"
date: "2021/10/14"
output: github_document
---

## Homework 3

```{r set, echo = FALSE, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(dplyr)
library(ggplot2)
theme_set(theme_minimal())
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)
options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

__Load and describe instacart data set.__

```{r load_data}
data("instacart")
```

The instacart data set contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. The data set is in a "long" format.The variable contains `r ls(instacart)`. Every user has an unique user id, and each order has an unique order id. Every product was assigned a product id, the aisle together with aisle id showed the category of product. Product belongs to different department and could distinguished by department id. Reorder indicated whether the product has been ordered before, with 1 means yes, 0 means no. Order dow and order hour of day means at which hour in the weekday it was ordered. Take row one as an example.

```{r, echo = FALSE}
head(instacart, num = 3) %>% 
  knitr::kable()
```


Row one means in order 1, user 112108 add 4 boxes of Bulgarian yogurt first in to the cart. This product was also ordered 10 days before. Bulgarian yogurt was identified as 49302 and belongs to yogurt category (id 120) in dairy eggs department (id 16). The order was at 10 o'clock in Thursday.

__How many aisles are there, and which aisles are the most items ordered from?__
```{r aisle_count}
instacart %>% 
  pull(aisle_id) %>% 
  n_distinct()

instacart %>% 
  group_by(aisle_id, aisle) %>% 
  summarize(n_item = n()) %>% 
  arrange(desc(n_item))
```

There totally 134 aisles, fresh_vegetables(id:83) had most items ordered from.

__Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.__

```{r aisle_plot, message = FALSE}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_item = n()) %>% 
  filter(n_item > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n_item)) %>% 
  ggplot() +
  geom_bar(aes(x = n_item, y = aisle, fill = aisle),stat = 'identity', alpha = 0.5) +
  labs(
    title = "No. of items vs Aisle",
    y = "Aisle", 
    x = "No. of item",
  ) +
  scale_x_continuous( 
    breaks = c(10000, 20000, 40000, 80000, 120000)
    ) +
  theme(
    legend.position = 'none',
    axis.text.y = element_text(size = 6),
    axis.text.x = element_text(size = 5))
```


There are 39 aisles have more than 10000 items sold. The plots shows from the most item sold aisle to the least one above 10000. 


__Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.__

```{r popular3}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

The top 3 popular item in baking ingredients is Light Brown Sugar, Pure Baking Soda, Cane Sugar. The top 3 popular item in dog food care is Snake Sticks Chicken & Rice Recipe Dog treats, Organix Chicken & Brown Rice, Small Dog Biscuits. The top 3 popular item in packaged vegetables fruits is Organic Baby Spinach, Organic Raspberries, Organic Blueberries. According to table, we could found that the items number has great different between aisles.

__Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).__

```{r meantime, message = FALSE}
instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(order_dow, order_hour_of_day, product_name) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(meant = round(mean(order_hour_of_day), digit = 2)) %>% 
  mutate(
    order_dow = case_when(
      order_dow == 1 ~ "Monday",
      order_dow == 2 ~ "Tuesday",
      order_dow == 3 ~ "Wednesday",
      order_dow == 4 ~ "Thursday",
      order_dow == 5 ~ "Friday",
      order_dow == 6 ~ "Saturday",
      order_dow == 0 ~ "Sunday",
    )
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = meant
  ) %>% 
  knitr::kable()
```

The Table shows mean hour of the Coffee Ice Cream and Pink Lady Apples sale. It could find that the hour of Pink Lady Apples fluctuate a little during the week while the Coffee Ice Cream has higher mean in the weekdays.


## Problem 2

__Load and clean BRFSS data set.__

```{r brfss}
data("brfss_smart2010")
brfss_df =
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health",
         response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>% 
  mutate(
    response = 
      fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

__In 2002, which states were observed at 7 or more locations? What about in 2010?__

```{r obe7, message = FALSE}
brfss_df %>% 
  filter(year %in% c("2002", "2010")) %>% 
  group_by(year, locationabbr) %>% 
  summarize(nloc = n_distinct(geo_location)) %>% 
  filter(nloc >= 7) %>% 
  knitr::kable()
```

According to the result, 6 states (CT FL NC MA NJ PA) were observed at 7 or more locations in 2002, while 14 states (CA CO FL MA MD NC NE NJ NY OH PA SC TX WA) were observed at 7 or more locations in 2010.

__Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.__

```{r excell, message = FALSE}
excell_df =
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  select(year, locationabbr, data_value) %>% 
  group_by(year, locationabbr) %>% 
  summarize(mean_data = mean(data_value))
  
```

__Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).__

```{r spaghetti, message = FALSE}
excell_df %>% 
  group_by(locationabbr) %>% 
  ggplot(aes(x = year, y = mean_data, color = locationabbr)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "The mean data value of state vs year",
    y = "Mean data value", 
    x = "Year",
    ) +
  scale_colour_hue("State abb") +
  theme(legend.key.size=unit(0.02,'cm')) +
  theme(legend.key.width=unit(0.05,'cm'))
```


The spaghetti plot shows the fluctuate of data_value together with year. From the plot, it could find that, in 2005, there was a decrease trend in data value, while it increased soon in the 2006. Though decreased again in 2007, in 2008, data value in most state increased again. Most of the value are between 20 and 25.


__Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.__

```{r 2006vs2010}
brfss_df %>% 
  filter(year %in% c("2006", "2010"),
         locationabbr == "NY") %>%
  group_by(year, response) %>% 
  ggplot(aes(data_value, fill = response, color = year)) +
  geom_density(alpha = 0.5)+
  facet_grid(.~ year) +
  labs(
    title = "The distribution of reponse vs location in NY state, 2006, 2010",
    y = "Density", 
    x = "Data value of response",
  )
  
  
```

The density plot shows the distribution of data value. In 2006, the response poor has higher density compared to 2010. The peak of other response all shift right in 2010 compared to 2006, which means higher data value in those response. 

## Problem 3

__Load and describe the data.__

```{r accel_df}
accel_df = 
  read_csv("./accel_data.csv", 
           col_types = cols(
             week = col_integer(),
             day_id = col_integer()
             )) %>%
  janitor::clean_names() %>% 
  mutate(
    weekend = case_when(
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend"
)) %>% 
  relocate("weekend") %>%
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity",
    values_to = "activity_count") %>% 
  separate(activity, into = c("activity", "activity_minute")) %>%
  select(-activity) %>%
  mutate(
    activity_minute = as.numeric(activity_minute),
    day = factor(day),
    day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))
```

The accel data set contains `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. The dataset contains 5 weeks of accelerometer data collected from a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The variables in this dataset are `r names(accel_df)`. day and day_id indicates the day the accelerometer data was collected. Week indicates which week the accelerometer data was collected. Weekend shows whether the information was collected on a weekend vs. a weekday. Activity_minute indicates the minute when the activity count was collected, corresponding to each minute of a 24-hour day starting at midnight.

__Aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?__

```{r activity_day}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_act = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_act
  ) %>% 
  knitr::kable()
```

It could find that the total activity on weekend in lower than the weekday.

__Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.__

```{r accel_plot}
accel_df %>% 
  ggplot(aes(x = activity_minute, y = activity_count, color = day)) + 
  geom_line(alpha = 0.3) +
  labs(
    title = "24-Hour Activity Count by Day",
    x = "Time",
    y = "Activity Count",
    caption = "Data from the accel dataset"
  ) + 
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440), 
    labels = c("12AM", "6AM", "12PM", "6PM", "11:59PM"),
    limits = c(0, 1440)
    )
```


It could find that the activity was low during 12AM to 6AM. There was a peak of activity around 12PM and 8PM.
































